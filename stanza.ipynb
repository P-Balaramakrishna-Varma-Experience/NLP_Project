{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdIse3aNSjbp",
        "outputId": "ab570826-66bd-431a-b581-2421c5f776e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from stanza) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.65.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza) (16.0.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=cecac0de32690010a31ca6a365efea194a507791810ba947c75be7b2c34da350\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.2.0 stanza-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('en') # download English model\n",
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
        "doc = nlp(\"I feel a energetic\") # run annotation over a sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GVBbU5aZSmoj",
        "outputId": "82b8b07a-519b-4d33-a3b0-854bb35bf011"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50b27f1c2165>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# download English model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initialize English neural pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I feel a energetic\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run annotation over a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb_EHy1XStIh",
        "outputId": "2f2ccb5d-31ac-4519-9727-0dc7edc03eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import conllu\n",
        "import torch\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_vocabularies(data_file):\n",
        "    pos_tags = [ \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\"]\n",
        "    pos_tags = [[i] for i in pos_tags]\n",
        "    assert(len(pos_tags) == 17)\n",
        "    vocab_pos_tags = torchtext.vocab.build_vocab_from_iterator(pos_tags)\n",
        "    vocab_pos_tags.append_token(\"pad\")\n",
        "    \n",
        "    dep_typ = [\"nsubj\", \"obj\", \"iobj\", \"csubj\", \"ccomp\", \"xcomp\", \"obl\", \"vocative\", \"expl\", \"dislocated\", \"advcl\", \"advmod\", \"discourse\" , \"aux\", \"cop\", \"mark\",\t \"nmod\", \"appos\", \"nummod\", \"acl\", \"amod\", \"det\", \"clf\", \"case\", \"conj\", \"cc\",\t \"fixed\", \"flat\", \"compound\", \"list\", \"parataxis\", \"orphan\", \"goeswith\", \"reparandum\", \"punct\", \"root\", \"dep\"]\n",
        "    dep_typ = [[i] for i in dep_typ]\n",
        "    assert len(dep_typ) == 37\n",
        "    vocab_deptyp = torchtext.vocab.build_vocab_from_iterator(dep_typ)\n",
        "    vocab_deptyp.append_token(\"pad\")\n",
        "\n",
        "    words = [[\"<unk>\"], [\"<unk>\"]]\n",
        "    TokenLists = conllu.parse_incr(open(data_file, \"r\", encoding=\"utf-8\"))\n",
        "    for TokenList in TokenLists:\n",
        "        for token in TokenList:\n",
        "            words.append([token[\"form\"]])\n",
        "    vocab_words = torchtext.vocab.build_vocab_from_iterator(words, min_freq=2)\n",
        "    vocab_words.set_default_index(vocab_words[\"<unk>\"])\n",
        "    #vocab_words.append_token(\"pad\")\n",
        "    return vocab_pos_tags, vocab_deptyp, vocab_words\n",
        "\n",
        "\n",
        "class DepData(Dataset):\n",
        "    def __init__(self, data_file, vocab_pos_tags, vocab_deptyp, vocab_words):\n",
        "        super().__init__()\n",
        "        self.vocab_pos_tags = vocab_pos_tags\n",
        "        self.vocab_deptyp = vocab_deptyp\n",
        "        self.vocab_words = vocab_words\n",
        "\n",
        "        Data = []\n",
        "        TokenLists = conllu.parse_incr(open(data_file, \"r\", encoding=\"utf-8\"))\n",
        "        for TokenList in TokenLists:\n",
        "            data = []\n",
        "            for token in TokenList:\n",
        "                if type(token[\"id\"]) == int:\n",
        "                    data.append([token[\"id\"], token[\"form\"], token[\"upos\"], token[\"head\"], token[\"deprel\"].split(\":\")[0]])\n",
        "                else:\n",
        "                    pass\n",
        "            #data = torch.LongTensor(data)\n",
        "            Data.append(data)\n",
        "        self.Data = Data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.Data[index]\n",
        "\n",
        " \n",
        "def custom_collate(batch):\n",
        "    return pad_sequence(batch, batch_first=True)\n",
        "\n",
        "vocab_pos_tags, vocab_deptyp, vocab_words = build_vocabularies(\"./en_ewt-ud-test.conllu\")\n",
        "\n",
        "data_t = DepData(\"./en_ewt-ud-test.conllu\", vocab_pos_tags, vocab_deptyp, vocab_words)\n",
        "#data_loader = DataLoader(data_t, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
        "\n",
        "\n",
        "# print(vocab_deptyp[\"pad\"], vocab_pos_tags[\"pad\"], vocab_words[\"pad\"])\n",
        "# data_2 = DepData(\"UD_English-EWT-master/en_ewt-ud-test.conllu\", vocab_pos_tags, vocab_deptyp, vocab_words)\n",
        "# data_3 = DepData(\"UD_English-EWT-master/en_ewt-ud-dev.conllu\",  vocab_pos_tags, vocab_deptyp, vocab_words)\n",
        "# for x in data_3_loader:\n",
        "#     pass"
      ],
      "metadata": {
        "id": "FMy5YyKoS6vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_t[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey6ITXIVS-sx",
        "outputId": "e0877061-5df8-49d8-efed-10441102dc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 'What', 'PRON', 0, 'root'], [2, 'if', 'SCONJ', 4, 'mark'], [3, 'Google', 'PROPN', 4, 'nsubj'], [4, 'expanded', 'VERB', 1, 'advcl'], [5, 'on', 'ADP', 15, 'case'], [6, 'its', 'PRON', 15, 'nmod'], [7, 'search', 'NOUN', 9, 'compound'], [8, '-', 'PUNCT', 9, 'punct'], [9, 'engine', 'NOUN', 15, 'compound'], [10, '(', 'PUNCT', 9, 'punct'], [11, 'and', 'CCONJ', 13, 'cc'], [12, 'now', 'ADV', 13, 'advmod'], [13, 'e-mail', 'NOUN', 9, 'conj'], [14, ')', 'PUNCT', 15, 'punct'], [15, 'wares', 'NOUN', 4, 'obl'], [16, 'into', 'ADP', 22, 'case'], [17, 'a', 'DET', 22, 'det'], [18, 'full', 'ADV', 20, 'advmod'], [19, '-', 'PUNCT', 20, 'punct'], [20, 'fledged', 'ADJ', 22, 'amod'], [21, 'operating', 'NOUN', 22, 'compound'], [22, 'system', 'NOUN', 4, 'obl'], [23, '?', 'PUNCT', 4, 'punct']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sentences=[]\n",
        "\n",
        "with open('./en_ewt-ud-test.conllu', 'r') as f:\n",
        "  for line in f:\n",
        "    tokens=line.split(\" \")\n",
        "    if tokens[0]==\"#\":\n",
        "      if tokens[1]==\"newdoc\" or tokens[1]==\"sent_id\" or tokens[1]==\"newpar\":continue\n",
        "      line=re.sub(\"# text = \",\"\",line)\n",
        "      sentences.append(line)\n",
        "total=0\n",
        "uas=0\n",
        "las=0\n",
        "for i in range(0,len(sentences)):\n",
        "  pred=nlp(sentences[i]).to_dict()\n",
        "  target=data_t[i]\n",
        "  if len(pred[0])==len(target):\n",
        "    print(f\"{len(pred[0])} {len(target)} {sentences[i]}\")\n",
        "    for j in range(len(pred[0])):\n",
        "      if pred[0][j][\"text\"]==target[j][1]:\n",
        "        if pred[0][j][\"head\"]==target[j][3]:\n",
        "          uas+=1\n",
        "          if pred[0][j][\"deprel\"]==target[j][4]:\n",
        "            las+=1\n",
        "        total+=1 #its not +1 , its +len(sentence)\n",
        "\n",
        "\n",
        "  #compare pred, target \n",
        "  #target - [id, word, pos, head,rel_type]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lYuw14pgTA3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(uas)\n",
        "# print(las)\n",
        "print('LAS score:')\n",
        "print((las/total)*100)\n",
        "# print(total)\n",
        "print('UAS score:')\n",
        "print((uas/total)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cZnPSKmTDwu",
        "outputId": "87569a8e-4d83-41bc-8b93-345b696d5680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAS score:\n",
            "89.1461255617595\n",
            "UAS score:\n",
            "91.31145308457033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMNXOZntWlka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}